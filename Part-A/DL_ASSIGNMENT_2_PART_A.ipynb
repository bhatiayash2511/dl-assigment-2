{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install wget\n","# !pip install tqdm"]},{"cell_type":"markdown","metadata":{},"source":["# Importing Necessary Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","from torchvision.datasets import ImageFolder\n","import torch.nn as nn\n","import torch\n","import torch.optim as optim\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import datasets, transforms\n","from torchvision.transforms import RandomHorizontalFlip, Resize, ToTensor, Compose, RandomRotation, RandomResizedCrop, RandomVerticalFlip\n","import os\n","from PIL import Image\n","torch.manual_seed(50)\n","random.seed(50)\n","np.random.seed(50)"]},{"cell_type":"markdown","metadata":{},"source":["# Check if Cuda is available or not"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["# Wandb login"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import wandb\n","!wandb login f6b40fc0bcc13c1d6117718b14ef1ddd1c68a700"]},{"cell_type":"markdown","metadata":{},"source":["# Implementing Convolutional Neural Network and Batch Normalization Implemented "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ConvNet(nn.Module):\n","    def __init__(self, num_filters, size_filters, activation_function, dense_neurons, img_size, get_paramters, batch_normalization, dropout_prob, padding = 0, stride = 1, padding_pool = 0, size_filter_pool = 2, stride_pooling = 2):\n","        self.batch_normalization = batch_normalization\n","        super(ConvNet, self).__init__()\n","        self.conv1 = nn.Conv2d(3, num_filters[0], size_filters[0])\n","        if(self.batch_normalization):\n","            self.bn1 = nn.BatchNorm2d(num_filters[0])  # BatchNorm after first Conv2d\n","        self.conv2 = nn.Conv2d(num_filters[0], num_filters[1], size_filters[1])\n","        if(self.batch_normalization):\n","            self.bn2 = nn.BatchNorm2d(num_filters[1])  # BatchNorm after second Conv2d\n","        self.conv3 = nn.Conv2d(num_filters[1], num_filters[2], size_filters[2])\n","        if(self.batch_normalization):\n","            self.bn3 = nn.BatchNorm2d(num_filters[2])  # BatchNorm after third Conv2d\n","        self.conv4 = nn.Conv2d(num_filters[2], num_filters[3], size_filters[3])\n","        if(self.batch_normalization):\n","            self.bn4 = nn.BatchNorm2d(num_filters[3])  # BatchNorm after fourth Conv2d\n","        self.conv5 = nn.Conv2d(num_filters[3], num_filters[4], size_filters[4])\n","        if(self.batch_normalization):\n","            self.bn5 = nn.BatchNorm2d(num_filters[4])  # BatchNorm after fifth Conv2d\n","\n","        self.flatten = nn.Flatten()\n","        self.dropout = nn.Dropout(dropout_prob)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.activation = nn.ReLU()  # Default activation is ReLU\n","        \n","        if activation_function.lower() == 'relu':\n","            self.activation = nn.ReLU()\n","        elif activation_function.lower() == 'sigmoid':\n","            self.activation = nn.Sigmoid()\n","        elif activation_function.lower() == 'tanh':\n","            self.activation = nn.Tanh()\n","        elif activation_function.lower() == 'leaky_relu':\n","            self.activation = nn.LeakyReLU()\n","        elif activation_function.lower() == 'gelu':\n","            self.activation = nn.GELU()\n","        elif activation_function.lower() == 'silu':\n","            self.activation = nn.SiLU()\n","        elif activation_function.lower() == 'mish':\n","            self.activation = Mish()\n","        \n","        \n","        image_size = img_size\n","        for it in range(5):\n","            image_size = self.update_image_size(image_size, size_filters[it], padding, stride, padding_pool, size_filter_pool, stride_pooling)\n","            print(image_size)                   \n","        \n","        \n","\n","        self.fc1 = nn.Linear(num_filters[4]*image_size*image_size, dense_neurons)\n","        \n","        self.output_layer = nn.Linear(dense_neurons, 10)\n","    \n","    def update_image_size(self, w, f, p, s, pp, fp, sp):\n","        res = (w - f + 2*p) // s + 1\n","        res = (res - fp  + 2*pp) // sp + 1\n","        return res\n","    \n","    def forward(self, x):\n","        \n","        if(self.batch_normalization):\n","            x = self.pool(self.activation(self.bn1(self.conv1(x))))  # BatchNorm after first Conv2d\n","            x = self.pool(self.activation(self.bn2(self.conv2(x))))  # BatchNorm after second Conv2d\n","            x = self.pool(self.activation(self.bn3(self.conv3(x))))  # BatchNorm after third Conv2d\n","            x = self.pool(self.activation(self.bn4(self.conv4(x))))  # BatchNorm after fourth Conv2d\n","            x = self.pool(self.activation(self.bn5(self.conv5(x))))  # BatchNorm after fifth Conv2d\n","\n","        else:\n","            x = self.pool(self.activation(self.conv1(x)))\n","            x = self.pool(self.activation(self.conv2(x)))\n","            x = self.pool(self.activation(self.conv3(x)))\n","            x = self.pool(self.activation(self.conv4(x)))\n","            x = self.pool(self.activation(self.conv5(x)))\n","            \n","        x = self.flatten(x)                                 # Flatten for fully connected layers\n","        x = self.dropout(x)\n","        x = self.fc1(x)\n","        x = F.softmax(self.output_layer(x),dim=1)\n","        # print(\"code added for checking\")\n","        return x\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training model Function\n","## - Data Augmentation implemented \n","## - Train set is splited into Val and Train\n"," \n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_model(data_aug, wandLog, model, train_dir, val_dir, num_epochs, batch_size, learning_rate, optimizer_name, img_size):\n","    # Define transforms for data augmentation and normalization\n","    if data_aug:\n","        transform_train = transforms.Compose([\n","            transforms.Resize((img_size, img_size)),\n","            RandomRotation(10),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","#             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","    else:\n","        transform_train = transforms.Compose([\n","            transforms.Resize((img_size, img_size)),\n","            transforms.ToTensor(),\n","#             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","\n","    transform_val = transforms.Compose([\n","        transforms.Resize((img_size, img_size)),\n","        # transforms.CenterCrop(32),\n","        transforms.ToTensor(),\n","#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","    # Load the datasets using ImageFolder\n","    train_dataset = ImageFolder(train_dir, transform=transform_train)\n","    val_dataset = ImageFolder(val_dir, transform=transform_val)\n","\n","\n","    labels = train_dataset.classes\n","    train_set, val_set = random_split(train_dataset, [8000, 1999])\n","\n","    # Create data loaders for training and validation\n","    train_loader = DataLoader(train_set, batch_size=batch_size, num_workers = 4,shuffle=True)\n","    val_loader = DataLoader(val_set, batch_size=batch_size,num_workers = 4, shuffle=True)\n","    test_loader = DataLoader(val_dataset, batch_size=batch_size,num_workers = 4, shuffle=True)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    if optimizer_name.lower() == 'sgd':\n","        # print(\"SGD\")\n","        optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum = 0.9)\n","    elif optimizer_name.lower() == 'rmsprop':\n","        optimizer = optim.RMSprop(model.parameters(), lr = learning_rate, alpha = 0.99, eps = 1e-8)\n","        # print(\"RMSPROP\")\n","    elif optimizer_name.lower() == 'adagrad':\n","        optimizer = optim.Adagrad(model.parameters(), lr = learning_rate, lr_decay = 0, weight_decay = 0, initial_accumulator_value = 0, eps = 1e-10)\n","        # print(\"ADAGRAD\")\n","    else:\n","        optimizer = optim.Adam(model.parameters(), lr = learning_rate, betas = (0.9, 0.999), eps = 1e-8)\n","        # print(\"ADAM\")\n","    \n","    \n","    epoch = 0\n","    while epoch < num_epochs:\n","        model.train()  # Set model to training mode\n","        # count = 0\n","        running_loss, train_correct_p, train_total_p = 0.0, 0, 0\n","        \n","        for i, data in train_loader:\n","            # print(i)\n","            inputs = i.to(device)\n","            labels = data.to(device)\n","#             inputs, labels = i, data\n","            \n","            optimizer.zero_grad()\n","\n","            outputs = model(inputs)\n","            \n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss = running_loss + loss.item()\n","            _, pred = torch.max(outputs.data, 1)\n","            train_total_p += labels.size(0)\n","            train_correct_p += (pred == labels).sum().item()\n","            \n","        \n","        # count += 1\n","#         print(count)\n","        \n","        # Validate the model after each epoch\n","        \n","        model.eval()  # Set model to evaluation mode\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for i, data in val_loader:\n","                inputs, labels = i.to(device), data.to(device)\n","#                 inputs, labels = i, data\n","                outputs = model.forward(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","\n","        # Print Train Statistics\n","        running_loss = running_loss / len(train_loader)\n","        acur_on_scale_1 = train_correct_p / train_total_p\n","        train_accuracy = acur_on_scale_1 * 100\n","        print(f'Epoch {epoch+1}, Train Loss: {running_loss:.3f}, Train Accuracy: {train_accuracy:.2f}%')      \n","        # Print validation statistics\n","        val_loss = val_loss/len(val_loader)\n","        temp2 = correct / total\n","        val_accuracy = 100 * temp2\n","        print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.2f}%')\n","        print()\n","        if wandLog:\n","            wandb.log(\n","                {\n","                    'epoch': epoch+1,\n","                    'training_loss' : running_loss,\n","                    'training_accuracy' : train_accuracy,\n","                    'validation_loss' : val_loss,\n","                    'validation_accuracy':val_accuracy\n","                }\n","            )\n","        epoch += 1\n","    print('Training finished')\n","    return model, val_accuracy \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Sweep Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sweep_config = {\n","            'name': 'sweep 1 : bayes',\n","            'method': 'bayes',\n","            'metric': { 'goal': 'maximize','name': 'Accuracy'},\n","            'parameters': \n","                {\n","                    'epochs' : {'values': [25]},\n","                    'num_kernels' : {'values': [[16, 32, 64, 128, 256], [16, 16, 32, 32, 16], [16, 32, 16, 32, 16], [32, 32, 32, 32, 32]]},\n","                    'kernel_sizes' : {'values': [[11, 9, 7, 5, 3], [11, 9, 7, 7, 5], [7, 7, 5, 5, 3], [11, 11, 7, 7, 3]]},\n","                    'batch_size' : {'values': [32]},\n","                    'dense_sizes' : {'values': [256]},\n","                    'activation' : {'values': ['ReLU', 'LeakyReLU']}, \n","                    'optimizer' : {'values' : ['adam']},\n","                    'learning_rate' : {'values': [0.0001, 0.0003]},\n","                    'batch_norm' : {'values': [True]},\n","                    'dropout' : {'values': [0.2, 0.3]},\n","                    'img_size' : {'values': [256, 224]},\n","                    'data_aug' : {'values' : [False]}\n","                }\n","            }"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","def run_sweep():\n","    init = wandb.init(project = 'Deep Learning Assignment 2 Random Sweep')\n","    config = init.config\n","    \n","    batch_size = config.batch_size\n","    num_epochs = config.epochs\n","    learning_rate = config.learning_rate\n","    optimizer_name = config.optimizer\n","    dropout_prob = config.dropout\n","    batch_normalization = config.batch_norm\n","    img_size = config.img_size\n","    dense_neurons = config.dense_sizes\n","    activation_function = config.activation\n","    wandLog = 1\n","    num_filters = config.num_kernels\n","    size_filters = config.kernel_sizes\n","    get_paramters = True\n","    data_aug = config.data_aug\n","    \n","    train_dir = '/kaggle/input/nature/inaturalist_12K/train'\n","    val_dir = '/kaggle/input/nature/inaturalist_12K/val'\n","    \n","    wandb.run.name = (\n","        'ACTIV_:' + activation_function +\n","        'OP_:' + optimizer_name +\n","        'SF_:' + ','.join(map(str, size_filters)) +\n","        'NF_:' + str(num_filters) +\n","        'IMGSI_:' + str(img_size)+\n","        'LR_:' + str(learning_rate) +\n","        'E_:' + str(num_epochs) +\n","        'DN_:' + str(dense_neurons) +\n","        'BATCHS_:' + str(batch_size) +\n","        'BATCHN_:' + str(batch_normalization) +\n","        'ProbD_:' + str(dropout_prob) +\n","        'AUG_:' + str(data_aug)\n","    )\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    model = ConvNet(num_filters, size_filters, activation_function, dense_neurons, img_size, get_paramters, batch_normalization, dropout_prob).to(device)\n","#     model = ConvNet(num_filters, size_filters, activation_function, dense_neurons, img_size, get_paramters, batch_normalization, dropout_prob)\n","    \n","    model_after_train, v_a = train_model(data_aug, wandLog, model, train_dir, val_dir, num_epochs, batch_size, learning_rate, optimizer_name, img_size)      \n","    wandb.log({'Accuracy': v_a})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sweep_id = wandb.sweep(sweep_config, project='Deep Learning Assignment 2 Random Sweep')\n","wandb.agent(sweep_id, run_sweep, count = 30)\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4734881,"sourceId":8032752,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
